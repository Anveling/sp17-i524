\section{Deployment} \label{deploymentsection}

The deployment on the cluster can be accomplished using 2 steps, assuming the cluster
is up and running
\begin{itemize}[noitemsep]
\item Step1: update hosts file \textit{ansible-word2vec/hosts} with the IP of the master. 
The first node on the cluster becomes the master node.
\item Step2: run the script  \textit{ansible-word2vec/run.sh}. This script will run the ansible 
playbooks to accomplish stage1 through stage4 of the deployment process.
\end{itemize}

\begin{figure}[htbp]
\centering
\fbox{\includegraphics[width=\linewidth]{images/deployment.png}}
\caption{Deployment stages.}
\label{fig:deploymentstages}
\end{figure}

Figure \ref{fig:deploymentstages} shows the deployment stages. The two steps
accomplish deployment in multiple stages as discussed in the sections below.

\subsection{Stage1} \label{deploymentstage1} 
As pre-requisite, we need to create a cluster with 1 or more nodes.
We created a 3 node cluster using Cloudmesh \cite{www-cloudmesh} command line interface(CLI). 
Cloudmesh\cite{www-cloudmesh} CLI allows you to orchestrate virtual machines(VM) in a 
cloud environment. For this project, we have used Chameleon and Jetstream cloud 
providers to orchestrate the VMs using cloudmesh\cite{www-cloudmesh} CLI. We can orchestrate a 3 node 
cluster using following CLI:

\begin{verbatim}
cm reset
pip uninstall cloudmesh_client
pip install -U cloudmesh_client
cm key add --ssh
cm refresh on
cm cluster define --count 3 --image CC-Ubuntu14.04 --flavor m1.medium
cm hadoop define spark pig
cm hadoop sync
cm hadoop deploy
cm cluster cross_ssh
\end{verbatim}   

We are using Ubuntu14.04 image with m1.medium which comes with 2 CPU, 4GB
memory. Also, the nodes created are having hadoop and spark add-ons.
We can test the deployment by checking hdfs and spark-submit CLI work fine.
\begin{verbatim}
ssh cc@<cluster-ip>
sudo su - hadoop
hdfs
spark-submit
\end{verbatim}

At this stage our cluster is ready for further deployments.

\subsection{Stage2} \label{deploymentstage2}  
By default, cloudmesh installs spark 1.6 but our word2vec solution requires
spark 2.1. We need to upgrade spark on the cluster. In order to do so we can run 
\textit{install\_upgrade\_spark.yaml} ansible\cite{www-ansible} playbook. This will download and unpack spark2.1 
tar ball and further update the softlink to point to spark 2.1 folder.

\subsection{Stage3} \label{deploymentstage3} 
In this step, we upgrade the development libraries for python, and pip,
install python modules like wkipedia, request etc, download the code from git repo and
install it in \textit{/opt/word2vec} folder, set the folder permissions for the \textit{/opt/word2vec} folder
so that it can be executed by hadoop user. These steps can be achieved using 
\textit{word2vec\_setup.yaml} playbook. After completing this stage, we are ready for running
our word2vec solution on the cluster. 

\subsection{Stage4} \label{deploymentstage4} 
This stage primarily deals with submitting the jobs for various purpose. 
Before we submit the jobs, we need to make sure input folder are created on hadfs.
First, we run the crawler to download the training set and upload the data on hdfs.
Further we run various jobs to created model and find relations. Along with these jobs
we also run some monitoring jobs. The monitoring job queries spark metrics using 
\begin{verbatim}
http://${spark_master}:4040
\end{verbatim}

Stage4 steps can be accomplished using \textit{word2vec\_execute.yaml} playbook. 

At the end of stage4 we also fetch the execution results from the cluster along with the 
metrics of execution times at various stages. The output files are fetched into \textit{/tmp/word2vec\_results}
\begin{verbatim}
ls -lrt /tmp/word2vec_results
total 56
-rw-rw-r--  1 abhigup4  wheel   571 Apr 18 17:36 jobs.csv
-rw-rw-r--  1 abhigup4  wheel   418 Apr 18 17:36 executors.csv
-rw-rw-r--  1 abhigup4  wheel    89 Apr 18 17:36 app.csv
-rwxrwxr-x  1 abhigup4  wheel    43 Apr 18 23:43 stest.csv
-rwxrwxr-x  1 abhigup4  wheel   145 Apr 18 23:43 relationstest.csv
-rw-r--r--  1 abhigup4  wheel  1833 Apr 18 23:46 stestresult.csv
-rw-r--r--  1 abhigup4  wheel   844 Apr 18 23:47 relationsresult.csv
\end{verbatim}
Files \textit{jobs.csv, executors.csv, and app.csv} collect the execution time for various jobs. 
File \textit{relationsresult.csv} file collects the results for sample relations corresponding to
\textit{relationstest.csv}. Similarly \textit{stestresult.csv} collects the results corresponding to \textit{stest.csv}.

